{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import codecs\n",
    "import re\n",
    "import random\n",
    "import pickle as pkl\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in our cleaning procedure is to find the websites which recepie data is collected from. This helps us create a scraper per website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = os.listdir('recipePages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE : please don't run just load pickle\n",
    "website_map = dict({})\n",
    "i = 0\n",
    "for link in links:\n",
    "\n",
    "    f = codecs.open('./recipePages/' + link, 'r', 'utf-8', errors='ignore')\n",
    "    document = BeautifulSoup(f.read(),'html.parser')\n",
    "    try:\n",
    "        a = document.body.find('a')\n",
    "        website_name = a['href']\n",
    "        website_url = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', website_name)\n",
    "        \n",
    "        if website_url!=[]:\n",
    "            website_url = website_url[0]\n",
    "            if website_url in website_map:\n",
    "                website_map[website_url].append(link)\n",
    "            else:\n",
    "                website_map[website_url] = [link]\n",
    "        \n",
    "        if (i%10000 == 0) and (i>0) :\n",
    "            with open('./pickles/recepie_to_websites' + str(i) + '.pkl', 'wb') as f:\n",
    "                pkl.dump(website_map, f)\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "    except:\n",
    "           continue\n",
    "\n",
    "with open('./pickles/recepie_to_websites.pkl', 'wb') as f:\n",
    "    pkl.dump(website_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/recepie_to_websites.pkl', 'rb') as f:\n",
    "    website_map = pkl.load(f) # a map having for each url a list of all html files belonging to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12,12]\n",
    "website_distribution = dict()\n",
    "for key in website_map.keys():\n",
    "    website_distribution[key] = len(website_map[key])\n",
    "web_dist_df = pd.DataFrame(website_distribution.items())\n",
    "web_dist_df = web_dist_df.set_index(0)\n",
    "web_dist_df.plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_food_network(links):\n",
    "    \n",
    "    final_res = []\n",
    "    applied = lambda x: x.contents[0] if isinstance(x,bs4.element.Tag) else x\n",
    "    \n",
    "    for link in links:\n",
    "        \n",
    "        try:\n",
    "            f = codecs.open('./recipePages/' + link, 'r', 'utf-8', errors='ignore')\n",
    "            document = BeautifulSoup(f.read(),'html.parser')\n",
    "\n",
    "            recepie_name = document.find('h1',{'class':'fn'}).contents[0]\n",
    "\n",
    "            category = map(lambda x : x.contents [0] ,document.find(\"div\", {\"class\": \"more\"}).find('ul').findAll('a'))\n",
    "\n",
    "            rating_info = int(document.find(\"div\",{\"class\":\"rm-block lead hreview-aggregate review\"}).find('div')['title'])\n",
    "\n",
    "            preparation_time = document.find(\"div\",{\"class\":\"rm-block-wrap\"})\n",
    "            prep_time = re.findall('\\d+',preparation_time.find('dd',{'class':'prepTime clrfix'}).find('span').contents[0])[0]\n",
    "            cook_time = re.findall('\\d+',preparation_time.find('dd',{'class':'cookTime clrfix'}).find('span').contents[0])[0]\n",
    "            diff_level = document.findAll('dd',{'class':'clrfix'})[-1:][0].contents[0]\n",
    "\n",
    "            reviews = map(lambda x : x.contents[0].strip(),document.findAll('p',{'class':'quote'}))\n",
    "            review_dates = document.findAll('div',{'class':'about-recipe-info'})\n",
    "            review_dates = map(lambda x : x.findAll('p')[-1].contents[0][2:],review_dates)\n",
    "            \n",
    "            review_info = list(zip(reviews,review_dates))\n",
    "            \n",
    "            for review in review_info:\n",
    "\n",
    "                data = {\n",
    "                    'name':recepie_name,\n",
    "                    'category':category,\n",
    "                    'review':review[0],\n",
    "                    'review_date':review[1],\n",
    "                    'rating':rating_info,\n",
    "                    'prep_time':prep_time,\n",
    "                    'cook_time':cook_time,\n",
    "                    'difficulty':diff_level\n",
    "                }\n",
    "                final_res.append(data)\n",
    "        \n",
    "        except: \n",
    "            continue\n",
    "    \n",
    "    return final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "food_net_result = parse_food_network(website_map['https://subscribe.hearstmags.com'])\n",
    "with open('./pickles/food_net.json', 'w') as fout:\n",
    "    json.dump(food_net_result, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#NOTE : you can use this code to open files of the dataset in the browser\n",
    "'''\n",
    "import webbrowser\n",
    "new=2\n",
    "urls = [website_map['https://subscribe.hearstmags.com'][0]]\n",
    "for link in urls:\n",
    "    webbrowser.open('recipePages/'+ link,new=new)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_allrecepies(links):\n",
    "    \n",
    "    final_res = []\n",
    "    \n",
    "    for link in links:\n",
    "        \n",
    "        try:\n",
    "            f = codecs.open('./recipePages/' + link, 'r', 'utf-8', errors='ignore')\n",
    "            document = BeautifulSoup(f.read(),'html.parser')\n",
    "\n",
    "            recepie_name    = document.find('span',{'class','itemreviewed'}).contents[0]\n",
    "            rating          = int(re.findall('\\d',document.find('p',{'class':'reviewP'}).find('img')['title'])[0])\n",
    "\n",
    "            serving_info    = document.find('div',{'class':'servings-form'}).find('input')['value']\n",
    "            \n",
    "            timing_data     = document.find('div',{'class':'times'})\n",
    "            if timing_data is not None:\n",
    "                prep_time  = timing_data.find('span',{'class':'prepTime'}).find('span')['title'][2:]\n",
    "                cook_time  = timing_data.find('span',{'class':'cookTime'}).find('span')['title'][2:]\n",
    "                total_time = timing_data.find('span',{'class':'totalTime'}).find('span')['title'][2:]\n",
    "            else:\n",
    "                prep_time  = -1\n",
    "                cook_time  = -1\n",
    "                total_time = -1\n",
    "                \n",
    "            \n",
    "\n",
    "            nutrition_facts  = document.find('div',{'id':'nutri-info'}).findAll('span')\n",
    "            nutrition_facts  = map(lambda x : {x['class'][0] : x.contents[0]} ,nutrition_facts)\n",
    "            nutrition_info   = dict()\n",
    "            for subDict in nutrition_facts:\n",
    "                key = subDict.keys()[0]\n",
    "                nutrition_info[key] = subDict[key]\n",
    "\n",
    "            reviews          = document.findAll('div',{'class':'listItemReviewMini'})\n",
    "            reviews          = map(lambda x : x.contents[0].strip(),reviews)\n",
    "            \n",
    "            review_date      = document.findAll('div',{'class','recreview'})\n",
    "            review_date      = map(lambda x : x.find('div',{'class','review'}).contents[0].split('\\n')[2].strip(),review_date) \n",
    "                                       \n",
    "\n",
    "            review_info = list(zip(reviews,review_date))\n",
    "                \n",
    "                \n",
    "            for review in review_info:\n",
    "                data = {\n",
    "                    'name': recepie_name,\n",
    "                    'rating': rating,\n",
    "                    'servings':serving_info,\n",
    "                    'review' : review[0],\n",
    "                    'prep_time':prep_time,\n",
    "                    'cook_time':cook_time,\n",
    "                    'total_time':total_time,\n",
    "                    'review_date':review[1]\n",
    "                }\n",
    "                data.update(nutrition_info)\n",
    "                final_res.append(data)\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recpie_result = parse_allrecepies(website_map['http://allrecipes.com'])\n",
    "with open('./pickles/all_recpie11.json', 'w') as fout:\n",
    "      json.dump(all_recpie_result, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_food_com(links):\n",
    "    \n",
    "    final_res = []\n",
    "    \n",
    "    for link in links:\n",
    "\n",
    "        try:\n",
    "            f = codecs.open('./recipePages/' + link, 'r', 'utf-8', errors='ignore')\n",
    "            document = BeautifulSoup(f.read(),'html.parser')\n",
    "\n",
    "            recepie_name     = document.find('h1',{'itemprop':'name'}).contents[0]\n",
    "\n",
    "            document_posted  = document.find('div',{'class':'addedwhen clrfix'}).find('meta')['content']\n",
    "\n",
    "            categories       = document.find('p',{\"class\",\"recipe-cats\"}).findAll('span',{'itemprop':'recipeCategory'})\n",
    "            categories       = map(lambda x: x.contents[0],categories)\n",
    "\n",
    "            preparation_time = document.find('div',{'class':'ckTime clrfix'}).find('div',{'class':\"ct-e\"}).findAll('p')\n",
    "            preparation_time = map(lambda x : x.contents,preparation_time)\n",
    "            preparation_time = map(lambda x: {x[0]['itemprop'] : x[1]},preparation_time)\n",
    "\n",
    "            num_servings     = document.find('select').find('option',{'class':\"select-title\"}).contents[0]\n",
    "\n",
    "            nutrition_facts  = document.find('div',{'itemprop':'nutrition'}).findAll('span')\n",
    "            nutrition_facts  = filter(lambda x : x.has_attr('itemprop'),nutrition_facts)\n",
    "            nutrition_facts  = map(lambda x : {x['itemprop']:x.contents[0]},nutrition_facts)\n",
    "            resulting_facts  = dict()\n",
    "            for subdict in nutrition_facts:\n",
    "                key = subdict.keys()[0]\n",
    "                resulting_facts[key] = subdict[key]\n",
    "\n",
    "            \n",
    "            user_reviews     = document.findAll('span',{'itemprop':'description'})\n",
    "            user_reviews     = map(lambda x : x.contents[0],user_reviews)\n",
    "            \n",
    "            review_dates     = document.findAll('li',{'itemprop':'reviews'})\n",
    "            review_dates     = map(lambda x: x.find('meta')['content'],review_dates)\n",
    "\n",
    "            review_info      = list(zip(user_reviews,review_dates))\n",
    "            \n",
    "            \n",
    "            for review in review_info:\n",
    "\n",
    "                data = {\n",
    "                    'name'    : recepie_name,\n",
    "                    'date'    : document_posted,\n",
    "                    'category': categories,\n",
    "                    'review'  : review[0],\n",
    "                    'review_date': review[1],\n",
    "                    'servings': num_servings\n",
    "                }\n",
    "                data.update(preparation_time[0])\n",
    "                data.update(preparation_time[1])\n",
    "                data.update(resulting_facts)\n",
    "                final_res.append(data)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        \n",
    "    return final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_com_result = parse_food_com(website_map['http://www.food.com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/food_com7.json', 'w') as fout:\n",
    "    json.dump(food_com_result, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dirs = os.listdir('./pickles')[1:]\n",
    "data = []\n",
    "for dirs in json_dirs:\n",
    "    \n",
    "    sub_dir = os.listdir('./pickles/' + dirs + '/')\n",
    "    recepie_data = []\n",
    "    for json_file in sub_dir:\n",
    "        file_info = json.load(open('./pickles/' + dirs + '/' + json_file))\n",
    "        recepie_data = recepie_data + file_info\n",
    "    \n",
    "    df = pd.DataFrame(recepie_data)\n",
    "    data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].to_json('./pickles/all_recipe_df.json')\n",
    "data[1].to_json('./pickles/foo_com_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1ad762284d78022a59725dd865d276fd.html'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_map['http://www.food.com'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
